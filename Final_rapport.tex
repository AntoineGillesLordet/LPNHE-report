\documentclass{book}

\usepackage{tikz}
\usepackage[compat=1.1.0]{tikz-feynman}
\usetikzlibrary{angles, quotes}
\usetikzlibrary{calc}
\usetikzlibrary{external}
\usetikzlibrary{decorations.pathreplacing, shapes.misc}
\usetikzlibrary{decorations.markings}
\tikzexternalize[prefix=tikz/]
\tikzset{external/system call={lualatex \tikzexternalcheckshellescape -halt-on-error -interaction=batchmode -jobname "\image" "\texsource"}}
\usepackage{shellesc}
\usepackage{subcaption}


\usepackage[backend=biber, style=apa]{biblatex}  
\addbibresource{biblio.bib}


\usepackage[english]{babel}
\usepackage[a4paper]{geometry}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17} 
\usepgfplotslibrary{fillbetween}

\usepackage{csquotes}
\usepackage{array}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage[normalem]{ulem}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{physics}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{lineno}
\usepackage{xspace}


\newcommand{\Ms}[0]{\textup{M}_\odot}
\newcommand{\todo}[1]{{\textcolor{gray}{\bf \large #1}}}
\newcommand{\consignes}[1]{{\textcolor{blue}{\bf \large #1}}}

\def\lemaitre{\textsc{Lemaître}\xspace}
\def\skysurvey{\texttt{skysurvey}\xspace}
\def\pets{\texttt{PETS}\xspace}
\def\nacl{\texttt{NaCl}\xspace}
\def\edris{\texttt{EDRIS}\xspace}
\def\saltd{\texttt{SALT2.4}\xspace}



\let\mcl\mathcal
\let\ov\overline
\let\ra\rightarrow

\newcolumntype{C}{>{\centering\arraybackslash}X}

\title{Rapport final de stage de Recherche au LPNHE\\\vspace{.3em} \large Mesure de la croissance des structures avec les galaxies du DESI BGS et les supernovae de type Ia de ZTF~: vers une analyse jointe}
\author{Antoine Gilles--Lordet\\ \vspace{.1em} \small encadré par Pauline Zarrouk et Nicolas Regnault }
\date{}

\begin{document}

\maketitle

\tableofcontents

\chapter{Synthèse}

\consignes{Une synthèse d’une page (ou executive summary) présentant le problème posé et ses enjeux pour le client, les éléments clés de la démarche, les solutions apportées et les résultats obtenus, et ce, en français et en anglais}

La cosmologie moderne repose sur un modèle nommé $\Lambda$CDM, construit à partir de la relativité générale d'Albert Einstein, et supposant que l'univers contient environ 5\% de matière ordinaire, 25\% de matière noire qui n'interagit que gravitationnellement, et de 60\% d'énergie noire dont le rôle est d'accéléré l'expansion de l'Univers. Cette accélération est encore mal comprise, et des modèles alternatifs de gravitation l'explique sans faire intervenir cette dernière composante. Dans ce contexte, ce stage vise à déterminer le taux de croissance des structures dans l'Univers, car l'évolution temporelle de cette grandeur dépend directement du modèle considérée. Elle constitue donc un test de la relativité générale aux échelles cosmologiques.

Ce stage se base sur les données de deux relevés, le relevé \textit{Dark Energy Spectroscopic Instrument} (DESI) et le relevé \textit{Zwicky Transient Facility} (ZTF). DESI a pour objectif de cataloguer et cartographier les galaxies de notre Univers proche, en mesurant leurs positions angulaires et leur redshift par spectroscopie. Ces galaxies possèdent une vitesse propre par rapport à l'expansion de l'Univers car elles sont constamment attirées par les puits de potentiels gravitationnels. Ces vitesses particulières déforment alors la distribution des redshift par effet Doppler, et il est possible de mesurer le taux de croissance des structures à partir de cette distorsion. ZTF est quand à lui un programme d'observation visant à détecter les phénomènes transitoires, en particulier les \textit{supernovae} (SN) de type Ia. Ces SNe sont particulièrement intéressante puisqu'elles constituent des chandelles standards, c'est-à-dire que leurs luminosités et leurs distances sont corrélées, ce qui permet de reconstruire avec précision la vitesse de leur galaxies hôtes. Leur utilisation en combinaison avec un catalogue de galaxies permet d'améliorer considérablement la précision sur la mesure du taux de croissance des structures.

Lors de ce stage, j'ai assemblée une chaîne d'analyse cosmologique et je l'ai testée sur des simulations. Pour cela des SNe possédant des vitesses particulières sont générées selon un modèle cosmologique fixé, et des observations sont simulées à l'aide des logs d'observations de ZTF. La chaîne d'analyse permet alors de reconstruire les paramètres de ces SNe ainsi que leurs vitesses particulières, afin d'en déduire le taux de croissance des structures.

\chapter{Introduction}

\section{Contexte}

Le modèle standard de la cosmologie, nommé $\Lambda$CDM, suppose que la gravité est décrite à toutes les échelles par la relativité générale, et que les principales contributions à la gravité viennent de la matière noire froide (\textit{Cold Dark Matter}) et l'énergie noire (sous la forme d'une constante cosmologique $\Lambda$). Ces deux inconnues sont nécessaires pour décrire l'évolution de l'Univers ainsi que la croissance des structures aux grandes échelles, et permettent à ce jour de reproduire fidèlement les observations, en particulier l'expansion accélérée de l'Univers. Cependant des modèles alternatifs de gravité ont également été proposés pour expliquer cette accélération sans faire intervenir de constante cosmologique, et prédisent des évolutions temporelles différentes de celles de la relativité générale pour certaines quantités.

Parmi ces quantités, on trouve notamment le taux de croissance des structures en fonction du redshift\footnote{En français, \textit{décalage vers le rouge}, effets de dilatation des longueurs d'ondes des photons entre leur émission et leur observation sur Terre du à l'expansion de l'Univers. Le terme anglais sera utilisé plutôt que sa traduction pour alléger les propos.} $f(z)$, et l'écart-type du champ de densité dans une sphère de rayon 8$h^{-1}$Mpc, $\sigma_8$. Ces deux quantités sont intéressantes car on peut montrer les vitesses particulières des galaxies, c'est-à-dire leurs vitesses par rapport à l'expansion de l'univers, sont proportionnelles au produit $f(z)\sigma_8(z)$ et caractérisent ainsi la formation des structures observées dans l'univers.

En plus de la nature inconnue de la matière noire et de l'énergie noire, le modèle $\Lambda$CDM présente également des tensions sur certains paramètres. Une tension survient lorsque des analyses utilisant différents types d'observation obtiennent une différence significative de valeur de paramètres. La plus connue est la tension sur la constante de Hubble $H_0$\footnote{les mesures directes utilisant le fond diffus cosmologique donne $H_0 \sim 67$ km.s$^{-1}$.Mpc$^{-1}$, tandis que les mesures indirectes reposant sur des supernovae ou des phénomènes de lentillage gravitationnel tendent vers $H_0 \sim 73$ km.s$^{-1}$.Mpc$^{-1}$.}, mais il existe également une tension sur $\sigma_8$ (voir les valeurs en Table \ref{tab:fs8}. Il est donc particulièrement important de déterminer sa valeur avec précision.

\begin{table}
	\begin{tabular}{l|c}
		Relevé & valeur\\
		\hline\\
		Planck2018 (\cite{planck_collaboration_planck_2020}) & $\sigma_8 = 0.811\pm 0.006$\\
		DES Y3 + KIDS-1000 (\cite{dark_energy_survey_y3_2023}) & $\sigma_8 = 0.825^{+0.067}_{-0.073}$\\
		SDSS IV  (\cite{eboss_collaboration_completed_2021}) & $\sigma_8 = 0.85 \pm 0.03$\\
		DESI (\cite{chen_not_2024}) & $\sigma_8 = 0.878^{+0.089}_{-0.080}$
	\end{tabular}
	\caption{Mesures de $\sigma_8$ avec différentes sondes}
	\label{tab:fs8}
\end{table}

\subsection{Analyse du clustering des galaxies pour mesurer $f\sigma_8$}

Pour mesurer la croissance des structures, la méthode la plus commune se base sur les \textbf{\textit{Redshift Space Distorsion} (RSD)} (\cite{kaiser_clustering_1987}), des distorsions liées aux transformations des coordonnées comobiles vers l'espace des redshifts. Le redshift mesuré, par exemple par spectroscopie, ne contient pas uniquement le redshift cosmologique dû à l'expansion de l'univers, mais inclut également une contribution par effet Doppler due aux vitesses particulières des galaxies, c'est-à-dire leurs vitesses propres par rapport à l'expansion de l'Univers.

Ce terme supplémentaire déplace les galaxies dans l'espace des redshifts par rapport à l'espace comobile, et les corrélations spatiales du champ de densité deviennent alors anisotropes : le long de la ligne de visée, les galaxies semblent plus regroupées qu'orthogonalement à la ligne de visée. A grande échelle, l'amplitude de cette anisotropie étant proportionnelle au facteur de croissance des structures $f$ et à l'amplitude des fluctuations du champ de densité, représenté communément par $\sigma_8$ (l'écart-type du champ de densité dans une sphère de rayon 8$h^{-1}$Mpc), ces analyses du clustering des galaxies contraignent le paramètre composite $f(z)\sigma_8(z)$. De nombreuses analyses des distorsions dans l'espace des redshifts ont été effectuées à l'aide de relevés spectroscopiques, tels que 6dFGRS (\cite{beutler_6df_2012}), SDSS-MGS (\cite{howlett_clustering_2015}), FastSound (\cite{okumura_subaru_2016}), SDSS-III BOSS (\cite{alam_clustering_2017}) ou SDSS-IV eBOSS (\cite{eboss_collaboration_completed_2021}). Les dernières mesures atteignent une précision de l'ordre de 10\%, et sont compatibles avec la relativité générale.

Une autre méthode pour mesurer $f\sigma_8$ est de dériver le paramètre des mesures directes des vitesses particulières des galaxies. Les vitesses particulières peuvent être mesurées directement à condition de pouvoir mesurer indépendamment les redshifts et les distances absolues des galaxies. Des mesures précises des redshifts peuvent être obtenues par spectroscopie et des estimations des distances peuvent être obtenues en utilisant les corrélations entre les distances et d'autres observables, telle que la relation de Tully-Fisher pour les galaxies spirales (corrélation entre la vitesse radiale de ses étoiles et la luminosité totale de la galaxie, \cite{tully_new_1977}) ou la méthode du Plan Fondamental (\cite{djorgovski_fundamental_1987}). Ces corrélations sont utilisables uniquement pour des galaxies à des redshifts relativement faibles ($z < 0,1$) car les incertitudes augmentent rapidement avec le redshift. Les propriétés statistiques d'un échantillon de vitesses particulières peuvent ensuite être utilisées seules ou en combinaison avec un champ de densité de galaxies (\cite{adams_joint_2020, qin_redshift_2019, turner_local_2023}.

\subsection{DESI DR1}

\todo{PRESENTER DESI}

Cette méthode à été utilisée à partir de la simulation Uchuu (\cite{prada_desi_2023}) à 2.1 milliars de particules, peuplée avec des galaxies du \textit{DESI Bright Galaxy Survey} (BGS) qui est un set complet de galaxies lumineuses ($r < 19.5$ pour le sous-échantillon \textit{Bright}) à bas redshift ($z<0.4$) (\cite{hahn_desi_2023}), pour produire un univers statistiquement fidèle au notre. Le catalogue obtenu reproduit donc à la fois le champ de densité et le champ de vitesse, à partir duquel des SNe avec vitesses particulières peuvent être tirées.


\subsection{Analyse cosmologique avec des SN Ia pour mesurer $f\sigma_8$}

À bas redshift, un autre moyen d'obtenir les distances vient des supernovae (SNe) de type Ia (\cite{hoyle_nucleosynthesis_1960}). Ces SNe sont des explosions thermonucléaire de naine blanche dans un système binaire, riches en carbone et oxygène, et présentent la particularité d’avoir des conditions d'explosions très similaires les unes aux autres. En particulier, les évolutions temporelles de leur luminosité dans leurs référentiels propres sont très similaires. Le maximum de flux émis, environ 5 milliards de fois plus que le soleil, est corrélé à leurs durées et leurs couleurs intrinsèque selon deux lois, communément appelées \textit{brighter-slower} et \textit{brighter-bluer}. Les SNe plus brillantes présentent une décroissance plus lente (donc durent plus longtemps), et présentent un décalage de leurs longueurs d'ondes émises vers le bleu, qui n'est pas du à un effet de redshift.

Grâce à ces deux corrélations, il est possible de décrire leurs flux maximaux à l'aide de deux paramètres, nommés \textit{stretch}\footnote{Comme pour le redshift, le paramètre de \textit{stretch} n'a pas d'équivalent français utilisé dans la communauté, le terme anglais sera donc employé à la place.} et couleur, et d'exprimer leur flux maximal en fonction d'un flux moyen et de contributions liées au stretch et à la couleur.

Cette caractéristique rend les SNe Ia standardisables et en fait d'excellentes chandelles standard~: des événements astrophysiques dont la luminosité peut être modélisée, ce qui permet d'en déduire directement leurs distances. Les SNe Ia ont ainsi permis la découverte de l'accélération de l'expansion de l'Univers (\cite{perlmutter_cosmology_1998, riess_observational_1998}).

Pour exprimer cette standardisation, il est plus commode de travailler avec des magnitudes plutôt que des flux, définies par $M=\log_{10}(\mcl f)$. La standardisation des SN Ia est alors décrite par la formule de Tripp (\cite{tripp_two-parameter_1998})
\begin{equation}
\label{eq:tripp}
    M^*_{b,SN} = M_b - \alpha x_{1,SN} + \beta c_{SN} + \sigma_{int}
\end{equation}
où $M^*_{b,SN}$ est la magnitude absolue\footnote{\textit{i.e.} dans le référentiel propre} en bande B, $x_{1,SN}$ est le \textit{stretch}, $c_{SN}$ est la couleur, et $M_b$ est la magnitude absolue moyenne à $c=0$ et $x_1=0$ dans la bande B. Expérimentalement, on mesure $M_b=19.3$, ainsi que les coefficients $\alpha=0.14$ et $\beta=3.15$. En réalité, la standardisation n'est pas parfaite et une dispersion intrinsèque subsiste, représentée par $\sigma_{int}$. Cette dispersion est modélisée par une distribution gaussienne d'écart type $0.10$.

L'outil principal pour exploiter les SNe est le diagramme de Hubble, la représentation de la relation entre les distances des SNe, représentées par leurs magnitudes, avec leurs vitesses, représentée par leurs redshifts. Un tel diagramme est représenté en Figure \ref{fig:residues}. Un modèle peut alors être ajusté, et les résidus au modèle donnent accès aux vitesses particulières des SNe (\cite{davis_effect_2011}). En effet, les magnitudes des SNe dépendent de leurs redshift cosmologiques, dus à l'expansion de l'univers, et non de leurs redshift observé, qui inclue les vitesses particulières. Les SNe apparaissent alors comme décalées horizontalement par rapport au diagramme de Hubble, et leurs vitesses particulières sont contenues dans les résidus au modèle.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Residues.png}
    \caption{Diagramme de Hubble de SNe simulés. Les vitesses particulières selon la ligne de visée provoque un écart entre leur redshift cosmologique et le redshift observé, ce qui se traduit par un décalage horizontal des SNe par rapport à la ligne de base du diagramme de Hubble. Les SNe ayant une vitesse particulière positive s'éloignent de nous et nous apparaissent plus rouges, tandis que celles ayant une vitesse particulière négative nous apparaissent plus bleues. Crédit : B. Carreres}
    \label{fig:residues}
\end{figure}

Les analyses de clustering de galaxies sont moins performantes à bas redshift, car le volume étudié ne contient qu'un faible nombre de galaxies et cette statistique est insuffisante pour produire des analyses précises. Les SNe Ia constituent alors un bien meilleur traceur du champs de vitesses à bas redshift que les galaxies, et permettent de meilleurs contraintes sur la croissance des structures (cf Figure \ref{fig:fs8}).

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fs8.png}
    \caption{Prévisions des contraintes sur $f\sigma_8$ pour les relevés DESI (\cite{hahn_desi_2023}), Euclid (\cite{euclid_collaboration_euclid_2024}) et la combinaison de DESI avec des vitesses particulières de ZTF (\cite{carreres_growth-rate_2023}) ou LSST (\cite{howlett_2mtf_2017}). Credit : J. Bautista}
    \label{fig:fs8}
\end{figure}

\subsection{Le projet \lemaitre}

 La réalisation d'un diagramme de Hubble de nouvelle génération utilisant des lots encore non exploités de SN Ia des relevés \textit{Zwicky Transient Facility} (ZTF, \cite{bellm_zwicky_2018}), SuperNovae Legacy Survey (SNLS, \cite{pritchet_snls_2004}) et Hyper Suprime-Cam Subaru Strategic Programm (HSC-SSP, \cite{miyazaki_hyper_2012,aihara_hyper_2018}) est actuellement l’objet du projet \lemaitre au LPNHE. Ce projet vise à développer un nouveau pipeline d'analyse cosmologique des SN Ia à partir de zéro, adapté au traitement des volumes de données attendus des prochains programmes d’observation tels que LSST (\cite{the_lsst_dark_energy_science_collaboration_lsst_2021}). Ce projet utilisant des lots non exploités, la mesure des paramètres du modèle sera donc indépendante des mesures précédentes.

La partie analyse cosmologique du pipeline \lemaitre \footnote{Un module de calibration est présent en amont, mais n'est pas l'objet de ce stage et ne sera donc pas abordé.} (voir également la Figure \ref{fig:lemaitre_pipeline}) est composé des modules suivants~:
\begin{enumerate}
    \item Le module \pets (Preprocessing and sElection of a Training Sample) effectue une première sélection en éliminant les points de photométrie ainsi que les SNe mal échantillonnée. Il donne également une première évaluation des paramètre de standardisation des SNe, qui sert de point de départ pour NaCl. La principale difficulté que doit résoudre ce module est de proposer une sélection indépendante du redshift et du rapport signal sur bruit, car cela introduirait des biais dans la suite de l'analyse. Plus de détails sont disponibles en Annexe \ref{anx:pets}.
    \item Le module \nacl (Nouvel algorithme de Courbe de lumière) évalue ensuite les luminosités des SNe dans leurs référentiels propres, ainsi que leurs paramètres de standardisations en entraînant un modèle empirique. Il vise à ré-implémenter un modèle type \saltd ( adapté à l'analyse des volumes de données des relevés futures. De plus, les incertitudes sont propagées lors de l'entraînement, et ne nécessite pas leurs caractérisations a posteriori sur des simulations. Le fonctionnement détaillé de ce module est présenté en Annexe \ref{anx:nacl}.
    \item Le module \edris (Estimateur de Distance pour les Relevés Incomplets de Supernovae) utilise les paramètres de standardisations et leur matrice de covariance pour ajuster un modèle cosmologique, en prenant en compte des effets de sélection de manière analytique. Là encore, l'estimation des biais introduit par des effets de sélections sur les modèles est habituellement traitée a posteriori sur des simulations. Le détail de ce module est présenté en Annexe \ref{anx:edris}.
\end{enumerate}

\begin{figure}
\centering
\begin{tikzpicture}[-stealth, line width=1.2pt]
	\node[rectangle, draw] (calib) at (0,0) {Calibration};
	\node[rectangle, draw, below=of calib] (pets) {\pets};
	\node[rectangle, draw, below=of pets] (nacl) {\nacl};
	\node[rectangle, draw, below=of nacl] (edris) {\edris};
	\node[rectangle, draw, below=of edris,  align=center, color=green!70!black] (out) {Output \\ - $\mu_{obs}$ \\ - Cosmologie};
	
	\node[rectangle, draw, above=of calib, color=blue, align=center] (logs) {INPUT \\ Logs d'observation de ZTF};	
	\node[rectangle, draw, right=of logs, color=blue, align=center] (uchuu) {INPUT \\ Simulation Uchuu};
	\node[rectangle, draw, below=of uchuu, red!50!black] (skys) {\skysurvey};

	\draw (logs) -- (calib) -- (pets) -- (nacl) -- (edris) node[left=2pt, pos=0.5, align=center, black] {$X_0, X_1, c$ \\ $Cov(X_0,X_1,c)$};
	\draw (edris) -- (out);

	\begin{scope}[orange]
		\draw [dashed] (pets) -- (nacl) -- (edris);
		\path (logs) -- (skys) coordinate[pos=0.5] (A);
		\draw ([xshift=2pt]logs.south) |- (A) -| (skys);
		\draw (uchuu) -- (skys);
		\node[ellipse, draw, align=center, aspect=2] (lcsim) at (skys |- pets) {Courbes de \\ Lumière \\ simulées};
		\draw (skys) -- (lcsim) -- (pets);
	\end{scope}
	\node[rectangle, draw, align=center, red!50!black] (rec) at (lcsim |- edris) {Reconstruction \\ avec \saltd \\ (via \skysurvey/\texttt{sncosmo})};
	\draw (rec) edge node[above, align=center] {$x_0,x_1,c$} node[below, align=center] {$Cov(x_0,x_1,c)$} (edris);
	\draw[orange] (lcsim) -- (rec);
	\draw[orange] (rec) -- (edris);
\end{tikzpicture}
\caption{Chaîne d'analyse \lemaitre développée au LPNHE (représentée en noire), et travail d'intégration effectué (en orange). L'étape de calibration contient en réalité plusieurs modules, mais elle n'a pas été utilisée pour ce stage car les données simulées n'incluait pas d'effets de calibration. \skysurvey ne fait pas partie intégrante de \lemaitre, il s'agit d'un sofware indépendant}
\label{fig:lemaitre_pipeline}
\end{figure}

Aux paramètres de standardisations $(x_1, c)$ précédemment introduits, on ajoute également le paramètre d'amplitude $x_0 = 10^{0.4 M^*_b}$, car la magnitude intervient sous cette forme dans le modèle \saltd. Les paramètres d'amplitude et de stretch utilisés par \nacl seront en revanche notés $X_0$, $X_1$ et $C$, car du fait des différences dans les contraintes et du ré-entraînement (voir Annexe \ref{anx:nacl}), rien ne garantit que ces paramètres soient exactement les mêmes. Dans les faits, ce n'est d'ailleurs pas le cas, mais les modules de distance demeurent eux bien constants entre \saltd et \nacl.

\subsection{Analyse jointe }

Il est également possible de cumuler les deux analyses, en introduisant dans le clustering des galaxies les vitesses particulières des SNe. Une telle analyse $f\sigma_8$ a par exemple été menée dans \cite{boruah_bayesian_2022, stahl_peculiar-velocity_2021}. Ces analyses jointes permettent de meilleure contraintes sur $f\sigma_8$ à bas redshift que pour les analyses de RSD des galaxies seules (voir Fig. \ref{fig:fs8}), et sont donc particulièrement intéressante obtenir des mesure de précision.

\section{Objectifs}

L'objectif du stage est double~:
\begin{enumerate}
    \item Produire des échantillons simulés de SNe basés sur les positions des galaxies de la simulation Uchuu qui reproduit les données spectroscopiques du DESI BGS, ainsi que les observations de ZTF, et les utiliser pour tester le pipeline \lemaitre actuellement en développement au LPNHE pour produire les diagrammes de Hubble et reconstruire la cosmologie sous-jacente des échantillons. Le pipeline étant encore en développement, je développe l'intégration des différents modules et je les teste.
    \item Comparer les résultats de ce pipeline, \textit{i.e.} les positions ainsi que les vitesses particulières reconstruites, aux données d'entrée et les utiliser pour
    	\begin{enumerate}
		\item Estimer $f \sigma_8$ à partir des vitesses particulières de SNe de ZTF et comparer ces résultats avec ceux de B. Carreres (\cite{carreres_growth-rate_2023}).
		\item Produire une analyse jointe du DESI BGS et des SNe de ZTF pour quantifier le gain sur les contraintes de $f\sigma_8$.
	\end{enumerate}
\end{enumerate}

\section{Enjeux}

L'enjeu majeur de ce stage est la validation du pipeline \lemaitre sur des simulations. En particulier, l'intégration faisant partie des tâches de ce stage, les différents modules n'ont été testés qu'individuellement sur des données simplifiées ou approximées. Les simulations de SNe utilisées et passées à travers tous les modules sont plus réalistes et permettront de mettre en lumière les points critiques dans l'exécution du pipeline. Ces points critiques incluent (sans être restreints à) le paramétrage des modules, des différences éventuelles de convention ou de nommage des variables utilisées, des erreurs dans les modèles, approximations ou dans leur implémentation et la caractérisation des biais potentiels.
Le fait de travailler sur des simulations permet un plus fin contrôle des conditions de test que les données réelles, et de facilement prendre en compte ou retirer différents effets.

Un second enjeu est l'estimation du gain offert par \lemaitre, et plus généralement des SNe de ZTF, dans l'estimation de $f\sigma_8$ par rapport à une analyse du seul clustering du DESI BGS. Comme évoqué précédemment, la détermination précise de ce paramètre est essentielle pour pouvoir valider ou invalider différents modèles de gravitation.

\section{Reformulation du problème dans son contexte}

L'estimation précise du paramètre $f\sigma_8$ est essentielle à la cosmologie actuelle. Elle permet a minima de contraindre les paramètres du modèle $\Lambda$CDM, et de valider ou invalider différents modèles de gravités dont la relativité générale afin d'améliorer notre compréhension de l'Univers. Cette estimation nécessite, à bas redshift, l'utilisation de traceurs plus performants que les seuls catalogues de galaxie du fait du faible volume exploré. Les données de SNe Ia peuvent fournir les vitesses particulières nécessaires à cette estimation, mais les relevés récents et futurs nécessiteront de traiter des volumes de données plus importants qu'auparavant. Dans ce contexte, le développement d'une nouvelle chaîne d'analyse adaptée est fondamental, et est l'objectif du pipeline \lemaitre. Ce pipeline nécessite cependant d'être testé sur des simulations afin de diagnostiquer d'éventuels problème ou biais, et de quantifier la précision sur les vitesses particulières reconstruites.


\chapter{Approche et analyse}

\section{État de l'art}
\consignes{méthodes, raisons des choix faits, analyses menées (état de l’art)}

\subsection{Mesurer $f\sigma_8$ avec le clustering des galaxies et les SN de type Ia}
\label{sec:art}
 Plusieurs méthodes ont été développées pour extraire des mesures du taux de croissance à partir de vitesses particulières~:
 \begin{itemize}
     \item la méthode dite du maximum de vraisemblance, où les champs de vitesse (et de densité) sont supposés être tirés de distributions Gaussiennes multivariées et sur lesquels un ajustement de $f\sigma_8$ est réalisé par maximum de vraisemblance (\cite{johnson_6df_2014, huterer_testing_2017, howlett_2mtf_2017, adams_joint_2020, lai_using_2022, carreres_growth-rate_2023});
     \item l'analyse de statistiques compressées à deux points telles que la fonction de corrélation à deux points, le spectre de puissance, ou les vitesses moyennes par paire sur lesquelles $f\sigma_8$ est ajusté (\cite{nusser_velocity-density_2017, dupuy_estimation_2019, qin_redshift_2019, turner_local_2023});
     \item la comparaison entre les vitesses observées et celles reconstruites à partir du champ de densité (\cite{davis_effect_2011, carrick_cosmological_2015, boruah_cosmic_2020, said_joint_2020, stahl_peculiar-velocity_2021}). En utilisant un modèle de gravitation, on peut exprimer le champ de vitesse à partir du champ de densité et du taux de croissance des structures. Cette relation peut alors être utilisée pour obtenir $f\sigma_8$ en faisant concorder les vitesse particulières observées et celles obtenues à partir du champ de densité;
     \item l'inférence dite \textit{fieldlevel}, qui consiste à retracer l'évolution complète de l'univers à partir de conditions initiales à l'aide d'un modèle, puis à comparer le résultat obtenu aux observations des champs de densité et vitesses, et rétro-propager les erreurs jusqu'au conditions initiales (\cite{boruah_bayesian_2022, prideaux-ghee_field-based_2023}). 
\end{itemize}

Un résumé des valeurs obtenues par ces diverses analyse est représenté en Fig. \ref{fig:carreres_11}. J'utilise pour l'analyse $f\sigma_8$ la méthode décrite dans \cite{boruah_cosmic_2020,stahl_peculiar-velocity_2021}, qui est de comparer les vitesses observées reconstruites avec des SN Ia aux vitesses déduites du champ de densité. Une description plus détaillée est donnée en Annexe \ref{anx:fs8}.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/Carreres_fig_11.png}
    \caption{Mesures du coefficient de croissance des structures $f\sigma_8$ à partir de vitesses particulières et de catalogues de galaxies. Les barres d'erreurs en trait fin inclues les erreurs systématiques, à l'exception de Dupuy et al. 2019, pour lequel la contribution supplémentaire vient de la variance cosmique. Credit : 
    \cite{carreres_growth-rate_2023}.}
    \label{fig:carreres_11}
\end{figure}

\section{Approche de résolution du problème}
\consignes{présentation détaillée de l’approche de résolution de problème retenue, des ressources scientifiques, techniques et humaines mises en oeuvre, de l’organisation du travail}

La méthode employée pour la génération suit celle de \cite{carreres_growth-rate_2023} pour les simulations, c'est à dire de tirer les paramètres de standardisations de distributions expérimentales, puis de générer des observations à partir de ces paramètres et des logs d'observations réels de ZTF. La reconstruction de vitesses particulières des SNe utilisera elle les nouveaux modules développés pour le projet \lemaitre. L'analyse $f\sigma_8$ prévue suit quand à elle celle de \cite{boruah_cosmic_2020,stahl_peculiar-velocity_2021}.

\subsection{Génération des SN Ia}
\label{sec:gen_SN}

Les vitesses particulières des SNe simulées ne peuvent pas être tirées aléatoirement, puisqu'elles doivent être en accord avec les structures de galaxies et les unes avec les autres. Une possibilité est de simuler la dynamique de la matière noire d'un univers via une simulation à N-corps, et de la peupler de galaxies en fonction de la distribution de densité obtenue. 

Dans un premier temps, un échantillon de SN Ia est généré selon les distributions observées des paramètres $x_0$, $x_1$, $c$ et $t_0$ (voir Table \ref{tab:snia}), et les positions et redshifts des galaxies BGS simulées dans Uchuu à l'aide de \skysurvey\footnote{\href{https://skysurvey.readthedocs.io/en/latest/}{https://skysurvey.readthedocs.io/en/latest/}}.

\begin{table}
    \centering
    \begin{tabular}{p{6.5cm}|p{7cm}}
         Paramètre & Distribution \\
         \hline
         Temps de maximum du flux en bande B $t_0$ & Uniforme entre le 02/03/2018 et le 01/01/2021\\
         Stretch $x_1$ & Gaussienne bimodale (\cite{nicolas_redshift_2021})\\
         Couleur $c$ &  Loi exponentielle convoluée avec une gaussienne (\cite{ginolin_ztf_2024}) \\
         Magnitude absolue $m_{abs}$ & Obtenue par la formule de Tripp\\
         Magnitude observée $m_{obs}$ & $m_{obs} = 5 \log(d_L(z_{cosmo})) + 25 + m_{abs}$\\
         Amplitude $x_0$ & $x_0 = 10^{0.4(m_b - m_{obs})}$
    \end{tabular}
    \caption{Distributions utilisées pour le tirage des paramètres des SN Ia. Il est important de noter que le redshift utilisé pour le calcul des magnitudes observées utilise le redshift cosmologique, et non le redshift observé.}
    \label{tab:snia}
\end{table}

Seules les galaxies à redshift $z<0.06$ sont utilisées afin de s'affranchir du biais de Malmquist (voir notamment \cite{carreres_growth-rate_2023} ou \cite{boyd_accounting_2024}). Ce biais observationnel vient du fait que les télescopes ont une magnitude limite d'observation, au-delà de laquelle les SNe ne sont plus détectées. Or, à un redshift donné, les SNe couvrent une plage de magnitude du fait de la dispersion due aux paramètres de standardisations, à la dispersion intrinsèque et aux vitesses particulières. Ainsi, à haut redshift, les SNe les moins brillantes ne sont pas observées, ce qui biaiserait une estimation des paramètres cosmologique. ZTF est considéré complet jusqu'à $z=0.06$, ce qui signifie que l'impact du biais de Malmquist en dessous de ce redshift est négligeable.

La période du 02/03/2018 au 01/01/2021 utilisée pour le tirage du temps de maximum correspond à la plage temporelle de la DR2 de ZTF\cite{rigault_ztf_2024}. Cette restriction sert à la fois à avoir un volume de donné proche de celui que devra traiter le pipeline \lemaitre pour estimer sa rapidité, et à pouvoir quantifier le gain qui pourrait être obtenu sur $f\sigma_8$ avec des données réelles. Le taux d'explosions de SN Ia est pris égal à $2,35 .10^4$Gpc$^{-3}$ (\cite{perley_zwicky_2020}).
Pour le calcul de la magnitude observée, $d_L(z)$ représente la distance de luminosité, qui est calculée à l'aide d'un modèle $\Lambda$CDM contraint par les données de Planck 2015 (\cite{planck_collaboration_planck_2016}) pour correspondre au modèle cosmologique utilisé par la simulation Uchuu.

Une simulation du relevé SNLS est également réalisée afin d'avoir un échantillon de SNe à plus haut redshift pour mieux contraindre le modèle cosmologique. Comme les redshifts observés par SNLS vont environ de $z=0.1$ à $z=1.2$, ils sont trop élévés pour le catalogue Uchuu BGS. Les positions des SNe sont donc tirées uniformément dans la footprint SNLS, et n'incluent pas d'effets de vitesses particulières. Seule la simulation de ZTF sera utilisée pour la reconstruction des vitesses particulières.

\subsection{Reconstruction des vitesses particulières}

Une fois les paramètres tirés, des points de données d'observations photométriques et spectroscopiques sont générés en utilisant le modèle de SNe \saltd (\cite{guy_salt2_2007, rigault_ztf_2024}) et les logs d'observations de ZTF. Ce modèle consiste en trois fonctions $M_0(p, \lambda)$, $M_1(p, \lambda)$ et $CL(\lambda)$ de manière à décrire les flux émis par les SNe dans leurs référentiels propres par~:
\begin{equation}
    F(SN, p, \lambda) = x_0 \times \qty[M^{\saltd}_0(p, \lambda) + x_1 M_1^{\saltd}(p, \lambda)] \times \exp[c CL^{\saltd}(\lambda)]
\end{equation}
où $p$, nommé phase, est le temps dans le référentiel de la SN depuis la date du maximum de luminosité dans la bande B, $\lambda$ est la longueur d'onde dans le référentiel de la SN, $x_0$ est l'amplitude du flux, et $x_1$ et $c$ sont les paramètres de standardisations de stretch et couleur. $M_0$ et $M_1$ sont appelés les modèles et $CL$ la loi de couleur.

Avec les résultats d'EDRIS, on peut remonter aux modules de distances
\begin{equation}
    \mu = m_b - M_b^*
\end{equation}
où $m_b$ est la magnitude maximale dans la bande B.
Comme les modules de distances ne dépendent que du redshift via le modèle cosmologique, en théorie $\mu = \mu(z)$ et tout les points devraient s'aligner sur le modèle (à une dispersion Gaussienne introduite par $\sigma_{int}$ près). En réalité comme mentionné en section \ref{sec:art}, les vitesses particulières ont pour effet de décaler les redshifts observés. Les points de données sont décalés horizontalement par rapport à la courbe théorique de $\mu(z)$ (voir Fig. \ref{fig:residues}).

On peut alors inverser cette courbe pour obtenir une relation $z(\mu)$, c'est-à-dire obtenir le redshift auquel correspond un module de distance, puis remonter aux vitesses particulières en utilisant~:
\begin{equation}
    v_p = c (z_{obs} - z(\mu))
\end{equation}
avec $c$ la vitesse de la lumière\footnote{à ne pas confondre avec la couleur. Comme il s'agit de l'unique utilisation de la vitesse de la lumière dans ce travail, la couleur continuera à être noté $c$}.

\subsection{Meilleure reconstruction}

Une meilleure reconstruction est possible en ré-entraînant le modèle lors de l'ajustement des paramètres. En effet, le modèle \saltd a été entraîné, puis les incertitudes évaluées sur des simulations. Or le modèle n'était pas nécessairement bien contraint sur l'intégralité de sa surface, et les incertitudes ne sont donc pas toujours fiables. Un ré-entraînement incluant la propagation des incertitudes devrait donc permettre d'améliorer la reconstruction du flux
\begin{equation}
    F(SN, p, \lambda) = X_0 \times \qty[M^{\nacl}_0(p, \lambda) + X_1 M^{\nacl}_1(p, \lambda)] \times \exp[0..4 C CL^{\nacl}(\lambda)]
\end{equation}


\todo{Avec réentraînement du modèle par \nacl, et intégration}



\section{Résultats intermédiaires et finaux visés}
\consignes{présentation des résultats intermédiaires et finaux visés}

\subsection{Résultats intermédiaires}

\subsubsection{Simulations avec vitesses particulières}

Le premier résultat visé est de générer des SNe à partir de la simulation Uchuu contenant l'information des vitesses particulières avec \skysurvey. Ce tirage doit reproduire la distribution en redshift des galaxies hôtes, et permettre une reconstruction des vitesses particulières à partir du diagramme de Hubble. Une fois les paramètres tirés, les courbes de lumière associées doivent être générées en utilisant les logs d'observations des relevés ZTF, SNLS et HSC.

\subsubsection{Reconstruction des SNe avec \saltd}

Le second résultat visé est de reconstruire les paramètres des SNe à partir de leurs courbes de lumière à l'aide de \saltd, et de caractériser cette reconstruction.

\subsubsection{Reconstruction de la cosmologie avec \edris et extraction des vitesses particulières}

Une fois les paramètres \saltd reconstruits, \edris sera exécuté afin d'obtenir la cosmologie sous-jacente et les coefficients de standardisation $\alpha$ et $\beta$.
Cela permettra alors d'obtenir les vitesses particulières via les résidus au diagramme de Hubble, et de caractériser les biais éventuels.

\subsubsection{Reconstruction des SNe avec \lemaitre}

En parallèle, le même résultat est attendu en utilisant \pets pour la sélection d'un lot d'entraînement puis l'entraînement du modèle et la reconstruction des paramètres des SNe avec \nacl. Cette deuxième reconstruction sera également injectée dans \edris afin d'obtenir les vitesses particulières, et caractériser les biais éventuels de \nacl.

\subsection{Résultats finaux}

Les résultats finaux visés contiennent la caractérisation des deux branches de reconstruction et leur comparaison, ainsi que l'exploitation des vitesses particulières reconstruites dans une analyse $f\sigma_8$ jointe Uchuu x ZTF.

\section{Difficultés rencontrées}
\consignes{exposé des difficultés rencontrées et de la façon dont elles ont été surmontées}

La principale difficulté que j'ai rencontré a été de suivre le développement des différents modules de \lemaitre tout en les utilisant et en remplissant mes tâches. Puisque l'intégration des modules n'était pas encore réalisée, j'ai pu identifier différents problèmes qui y étaient liés tels que des différences de convention ou de notation pour des variables. Mes tests ont également permis de révéler certains comportements anormaux des modules lors de leurs utilisations sur des données réalistes (par exemple des erreurs liées à des paramètres sortant des plages de définition du modèle dans NaCl), et de mettre en lumière des étapes de filtrages des données entre les modules afin d'assurer leurs bons fonctionnements. Cela a nécessité de faire des points pour discuter de l'avancement des modules en plus des points hebdomadaires prévus avec les autres membres de l'équipe, afin de remonter et résoudre les problèmes rencontrés, et être à jour sur l'utilisation des modules.

Une autre difficulté notable a été la compréhension fine du domaine et des tâches, afin de proposer une analyse correcte et robuste. J'ai parfois eu du mal à différencier certains concepts et outils, mais avec des explications et l'aide de mes encadrants et des autres chercheurs de l'équipe, j'ai pu surmonter cette difficulté.


\todo{Les accords pour les données DESI/ZTF/BORG}


\section{Gestion des risques réalisée}
\consignes{gestion des risques réalisée}

\todo{Temps de calcul -> optimisation, NERSC mais visée pour un ordi personnel}

\todo{Disponibilité des données}



\chapter{Résultats}
\consignes{présentation détaillée des résultats effectivement obtenus en les reliant au problème qui était à traiter et mettant en exergue :
\begin{itemize}
\item la validité des résultats par rapport au problème
\item le caractère innovant / neuf des résultats
\item l’utilité des résultats, l’usage effectif qui sera fait du travail de l’élève ingénieur
\item la mise en exergue des limites du travail et des suites à donner à ce travail
\item la quantification et/ou qualification de la valeur ajoutée
\end{itemize}}


\section{Validation de LEMAITRE sur simulations}

\subsection{Génération de SNe à partir d'Uchuu}

La distribution spatiale des SNe tirées est présentée en Fig. \ref{fig:z_draw} et \ref{fig:ang_draw}. La distribution en redshift observés (incluant les vitesses particulières) diffère bien des redshifts cosmologiques, et la distribution angulaire reproduit la footprint DESI Y5, qui est similaire à la footprint ZTF.

\begin{figure}
	\centering
	\begin{subfigure}[c]{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/redshift_draw.png}
		\caption{Distribution des redshift tirés à partir du catalogue de galaxies Uchuu. Le redshift cosmologique correspond au redshift de la position de la galaxie, le redshift observé inclue l'effet Doppler des vitesses particulières.}
		\label{fig:z_draw}
	\end{subfigure}
	\hfill
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim={1cm 2.5cm 1cm 2.5cm}, clip]{figures/angular_draw.png}
		\caption{Distribution angulaire des SNe tirées à partir du catalogue de galaxies Uchuu.}
		\label{fig:ang_draw}
	\end{subfigure}
	
	\caption{Distribution spatiale des SNe tirées}
\end{figure}

De plus il est déjà possible de quantifier l'effet de la distribution intrinsèque sur la reconstruction des vitesses particulières. En effet, une erreur commise sur les modules de distances impact directement le redshift cosmologiques reconstruit, et donc les vitesses particulières (cf. Fig \ref{fig:vp_sigma_int}). Cette erreur est corrélée avec le redshift, et devient plus importante à haut redshift car $\mu(z)$ y croît plus lentement.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{figures/sigma_int_effect_vp.png}
	\caption{Illustration de l'erreur commise à $1\sigma$ sur les redshifts cosmologiques reconstruits due à la dispersion intrinsèque.}
	\label{fig:vp_sigma_int}
\end{figure}

La distribution des vitesses particulières ainsi reconstruites est représentée en Fig. \ref{fig:vp_draw}. Les vitesses particulières sont reconstruites avec une erreur moyenne de $18.4$km/s et une dispersion de $596.8$km/s. On observe bien une augmentation de l'erreur commise sur les vitesses particulières reconstruites, et cette augmentation suit l'erreur théorique due à la dispersion intrinsèque. De plus, si on calcule la moyenne des erreurs théoriques due à la dispersion intrinsèque sur l'échantillon complet, on obtient $\sigma_{v_{pec}}(\sigma_{int}) = 592.4$km/s, ce qui est proche de la dispersion observée.

\begin{figure}
	\centering
	\includegraphics[width=0.9\textwidth]{figures/vp_draw.png}
	\caption{Distribution des vitesses particulières reconstruites directement à partir des SNe tirées à partir du catalogue de galaxies Uchuu. La ligne en pointillés noire (graphiques de gauche) représente la ligne de base $v_{pec, SN} = v_{pec, Uchuu}$, les lignes oranges (en bas à gauche) représente l'erreur théorique à $1\sigma$ due à la dispersion intrinsèque, et les lignes rouges pleine et en pointillés (en bas à droite) représentent respectivement la moyenne et la déviation standard des erreurs.}
	\label{fig:vp_draw}
\end{figure}

Une fois les paramètres des SNe tirés, les observations photométriques et spectroscopiques sont générées sur la base du modèle \saltd et des logs d'observation de ZTF. De telles courbes de lumière et spectres sont représentés en Figure \ref{fig:obs}.

\begin{figure}
	\centering
	\begin{subfigure}[c]{0.45\textwidth}
%		\includegraphics[width=\textwidth]{figures/redshift_draw.png}
		\caption{Courbes de lumière générées à partir des paramètres tirés. Les couleurs représentent les différents filtres utilisés.}
		\label{fig:lc}
	\end{subfigure}
	\hfill
	\begin{subfigure}[c]{0.5\textwidth}
		\centering
%		\includegraphics[width=\textwidth, trim={1cm 2.5cm 1cm 2.5cm}, clip]{figures/angular_draw.png}
		\caption{Spectre généré à partir des paramètres tirés}
		\label{fig:spectra}
	\end{subfigure}
	\label{fig:obs}
	\caption{Exemple d'observations simulées pour les échantillons de SNe}
\end{figure}

\subsection{Filtrage par \pets}

Le filtrage effectué par \pets permet d'enlever les SNe qui sont mal échantillonnées. Le nombre de SNe retirées à chaque étape de sélection est indiqué en Table \ref{tab:pets}, et suivent la démarche décrite en Annexe \ref{anx:pets}.

\begin{table}
    \centering
    \begin{tabular}{p{5cm}|c|c}
         Cut & Discarded & Remaining\\
         \hline
         Total & - & 1591\\
         \saltd converge & 442 & 1149\\
         $\sigma_{t_{max}}<1$ & 31 & 1118\\
         $\chi^2(t_{max})$ est symétrique autour de $t_{max}$ & 77 & 1041\\
         Pas d'autres minimums à $8\sigma$ & 9 & 1032\\
	$|x_1|<4$ & 0 & 1032\\
	$|c|<2$ & 0  & 1032
    \end{tabular}
    \caption{SNe rejetées par \pets à chaque étape de sélection pour le lot ZTF simulé}
    \label{tab:pets}
\end{table}

 Des cas de figure typiques sont représentés en Figures \ref{fig:pets_good}, \ref{fig:pets_bad} et \ref{fig:pets_worst}.
 
La première représente le cas d'une SNe bien échantillonnée~: le minimum de la fonction $\chi^2(t_{max}$ est unique, bien défini, et présente une incertitude de $0.012$ jours, ce qui est extrêmement faible. Cette précision est telle que les intervalles à $1\sigma$ et $3\sigma$ sont indiscernables du trait plein du $t_{max}$ déterminé, qui correspond parfaitement avec celui du fit initial. Les paramètres $x_1$ et $c$ ont également des valeurs raisonnables, la SN est donc conservée.

\begin{figure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{figures/276_lc_truth.png}
		\caption{Courbe de lumière simulée}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{figures/276_Tmaxgrid.png}
		\caption{Évolution du $\chi^2$ avec le temps de maximum}
	\end{subfigure}
	\caption{SNe validée par \pets. Le minimum du $\chi^2$ est unique et bien défini.}
	\label{fig:pets_good}
\end{figure}
 
Le second cas de figure (Figure \ref{fig:pets_bad}), est celui d'une SNe mal échantillonnée~: le minimum de la fonction $\chi^2(t_{max}$ est toujours unique, mais mal défini car le $\chi^2$ varie peu autour du minimum, ce qui a pour conséquence une incertitude de $^{+0.764}_{-0.828}$ jours et une asymétrie du $\chi^2$. Cette aplatissement du $\chi^2$ autour du minimum a pour conséquence majeure d'entraîner de fortes variations des fonctions $x_1(t_{max})$ et $c(t_{max})$, notamment vers $t_{max}=58 810$ où les deux paramètres sortent des plages acceptables. Ce comportement est à éviter pour l'entraînement de \nacl, car l'ajustement des paramètres du modèle conjointement à ceux des SNe a plus de chance de se bloquer et d'obtenir des paramètres de standardisations anormalement élevés. Cette SN est donc rejetée.

\begin{figure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{figures/10_lc_truth.png}
		\caption{Courbe de lumière simulée}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{figures/10_Tmaxgrid.png}
		\caption{Évolution du $\chi^2$ avec le temps de maximum}
	\end{subfigure}
	\caption{SNe rejetée par \pets. Le minimum du $\chi^2$ est unique mais mal défini.}
	\label{fig:pets_bad}
\end{figure}

Le troisième cas de figure concerne un cas pathologique, qui a été diagnostiqué lors de ce stage et est détaillé en Section \ref{sec:min_mult}. En effet, la SNe ne présente des points de photométrie que sur la fin de sa décroissance, ce qui a pour effet de focaliser la première estimation de $t_{max}$ autour de ces points. \pets ignorant alors les minimums aux extrémités de l'intervalle de $t_{max}$ utilisés, cette SNe ne présentait qu'un minimum bien défini, avec une incertitude faible de $\pm 0.111$ jours, et était donc satisfaisante. Ce comportement a donc été modifié pour prendre en compte les minimums aux extrémités, et la détection de minimums potentiellement problématiques remontée à $\Delta \chi^2 = 64$ (équivalent à une détection à $8\sigma$) plutôt que $\Delta \chi^2 = 9$ (respectivement $3\sigma$).


\begin{figure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{figures/544_lc_truth.png}
		\caption{Courbe de lumière simulée}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{figures/544_Tmaxgrid.png}
		\caption{Évolution du $\chi^2$ avec le temps de maximum}
	\end{subfigure}
	\caption{SNe anormale validée par \pets avant correction. Le minimum du $\chi^2$ trouvé initialement est un minimum local, et l'intervalle exploré ne couvre pas la vraie valeur. }
	\label{fig:pets_worst}
\end{figure}

\subsection{Reconstruction avec \saltd}

La première reconstruction possible une fois les courbes de lumières simulées à l'aide de \skysurvey est d'utiliser \saltd.
\todo{Chaine rapide}

%\begin{figure}
%	\centering
%	\includegraphics[width=0.9\textwidth]{figure/placeholder.png}
%	\caption{}
%	\label{fig:salt}
%\end{figure}


\subsection{Analyse cosmologique sur les résultats \saltd}


\subsection{Reconstruction et vitesses particulières avec \nacl}


\section{Problèmes identifiés dans le pipeline \lemaitre et corrections apportées}

J'ai identifié et traité différents problèmes et erreurs au sein du pipeline \lemaitre. Certains étaient triviaux à traiter une fois identifiés, tel que des erreurs dues à des différences d'encodage (sur 32 ou 64 bits) de certaines valeurs lors de sauvegardes et chargements de données entre des modules, d'autres plus compliqués, et quelques uns restent encore non résolus. Les problèmes rencontrés peuvent être regroupés en deux catégories~: ceux causés par l'utilisation de mauvaises valeurs de méta-paramètres, qui ne remettent pas en cause les méthodes mais nécessitent un ajustement de ces paramètres, et ceux liés directement aux méthodes utilisées.

\subsection{Problèmes liés à des valeurs de méta-paramètres}

\subsubsection{Minimums multiples dans \pets}
\label{sec:min_mult}

Un premier problème est apparu lors de l'utilisation de \pets concerne la caractérisation des SNe pour l'entraînement de \nacl. Certaines SNe ne présentent des points de mesure que sur la croissance ou la fin de la décroissance de leur luminosité pour des raisons observationnelles tel que leur concordance avec le début ou la fin d'une période d'observation. Ces SNe posent alors le problème d'être difficiles à caractériser, bien que parfaitement décrites par \saltd. En particulier, les SNe qui n'ont des points de photométrie que sur la fin de leur décroissance présentent souvent deux minimums locaux (voir Figure \ref{fig:pets_worst}). Comme \pets utilise le point de données de flux le plus élevé comme point de départ pour ajuster le $t_{max, 0}$ à partir duquel la grille est générée, il peut converger vers un minimum local bien marqué et générer une grille ne couvrant pas le minimum global au vrai temps de maximum.

La solution employée ici a été de remonter la détection des autres minimums de $3\sigma$ à $8\sigma$, et de retirer les SNe ainsi identifiées comme problématiques du lot de données à analyser. Une meilleure solution, implémentée pour les utilisations futures de \pets, est d'augmenter l'intervalle couvert par la grille, mais cela nécessite en contrepartie soit de diminuer sa résolution et donc l'estimation des incertitudes, soit d'utiliser plus de points, ce qui augmente le temps de calcul. A noter que si ces SNe sont bien décrites par \saltd, elles ne sont pas intéressantes pour entraîner \nacl puisqu'elles ne comportent que peu de point de données et ne couvrent qu'une faible fraction des phases. Elles pourraient être fittées après entraînement afin de gagner en statistiques, mais le gain restera minime puisqu'elles demeurent assez rares (environ ~1\% des SNe respectant par ailleurs les autres critères de \pets).

De plus c'est un problème qui est particulièrement présent dans mes simulations, mais devrait être rare lors de l'utilisation de données réelles. En effet, puisque seules des SNe Ia sont générées, elles sont reconstruites en partant du principe que ce sont des SNe Ia. Sur des données réelles, rien ne garantit qu'un événement transitoire dont on a observé uniquement la décroissance soit une SNe Ia. Il pourrait s'agir d'une SNe Ib, Ic, II, ou d'un autre type d'événement cataclysmique. Dans ce cas, l'événement ne sera pas classé avec certitude comme SN Ia, et ne passera pas donc par \pets.
 

\subsubsection{Domaine de définition des modèles}

Un deuxième problème qui est survenu vient du domaine de définition des modèles dans \nacl. Initialement, les modèles couvraient l'intervalle $[-20; 50]$ en phase et $[2000; 9000]$ Ångströms en longueur d'onde. Or, pour le calcul des flux des points de photométrie, il arrive que le support de la transmission du filtre sorte du domaine de définition en longueur d'onde des modèles. Cela revient à couper les filtres et fausse le flux calculé, ce qui en retour biaise la reconstruction des paramètres de standardisation puisque le modèle s'adapte pour compenser cette coupure.

La solution a été simplement d'augmenter le domaine de définition des modèles, de 2 000 à 11 000 Ångströms. Aux redshifts considérés, les filtres ne sortent plus du modèle et l'évaluation redevient correcte.

\subsubsection{Régularisation et contraintes dans \nacl}

Un autre problème identifié dans \nacl concerne les coefficients de régularisation $\mu_{reg}$ et de contraintes $\mu_{cons}$. Les premiers tests effectués utilisaient $\mu_{reg} = 1$ et $\mu_{cons} = 10^{-10}$, mais ces valeurs étaient trop faibles et ne permettaient pas d'imposer aux modèles les contraintes et la régularisation voulus. Une des conséquences majeures étaient une mauvaise convergence, et une surestimation des incertitudes sur les paramètres reconstruits.
Il s'est avéré que prendre des valeurs $\mu_{reg} = 100$ et $\mu_{cons} = 10^6$ donnent une bien meilleure convergence, et ne surestiment plus les incertitudes.

\subsubsection{Quality cuts pour \edris}
\label{sec:qc_edris}
Dès les premières tentatives de production de diagrammes de Hubble à partir de la reconstruction avec \saltd, la convergence d'\edris est apparue comme très dépendante des données. En particulier, la présence d'outliers est susceptibles de coincer le fit des paramètres et de la cosmologie. Les outliers peuvent apparaître soit lors de la génération (\textit{i.e.} les paramètres sont extrêmes mais probables au vu des distribution), soit lors de la reconstruction (\textit{i.e.} la SNe n'était pas aberrante, mais elle est mal reconstruite). L'amplitude des coefficients de la matrice de covariance des paramètres $(x_0, x_1, c)$ a un impact particulièrement important sur la convergence. C'est l'une des raisons qui a motivée l'exploration et la résolution du problème précédent~; une mauvaise convergence de \nacl donnait un point de départ aberrant pour la matrice de covariance pour \edris.

Cela a aussi conduit à appliquer des quality cuts semblables à ceux d'Union 3 aux paramètres reconstruits afin de retirer d'éventuels outliers ou SNe mal reconstruites.

\subsection{Problèmes liés aux méthodes}

\subsubsection{Effets d'une mauvaise convergence }

J'ai évoqué comme conséquences des problèmes précédents une mauvaise convergence des modules \nacl ou \edris. Dans les faits, cette mauvaise convergence est caractérisée par plusieurs symptômes, le plus problématique concernant la hessienne du problème $\mcl H$.

Comme il serait trop coûteux d'estimer les incertitudes tout au long de l'optimisation, la solution retenue pour le pipeline \lemaitre est d'utiliser la borne de Cramer-Rao comme estimateur de la covariance. À un coefficient multiplicatif près dépendant de la formulation exacte du problème, la covariance est obtenue en inversant la matrice d'information de Fisher, qui est ici égale la hessienne 
\begin{equation}
	Cov = \mcl I^{-1} = \mcl H^{-1}
\end{equation}
et les incertitudes sont directement obtenues en prenant les racines carrées des coefficients diagonaux
\begin{equation}
	\sigma_i = \sqrt{Cov_{ii}} = \sqrt{(\mcl H^{-1})_{ii}}
\end{equation}

Le problème qui survient alors est qu'en cas d'absence de convergence ou de mauvaise convergence, la hessienne peut ne pas être définie positive, ce qui rend les incertitudes indéterminées. Ce problème est survenu à de nombreuses reprises, bien souvent du à un mauvais point de départ ou des outliers.

Dans le cas de \nacl, cela a mis en lumière la nécessité d'avoir une bonne convergence dans la première estimation des paramètres des SNe. En cas de mauvaise estimation, le modèle essaie de surcompenser cette mauvaise estimation pour décrire les données lors du dernier fit. Comme il n'arrive pas à converger correctement, il se bloque généralement dans une région de l'espace des paramètres où la hessienne n'est pas définie positive, ce qui a permis d'identifier ce type de problèmes.

\subsubsection{Dégénérescence entre $\beta$ et $\sigma_{int}$} 

Il existe une dégénérescence entre le coefficient $\beta$ de la formule de Tripp et le paramètre de dispersion intrinsèque $\sigma_{int}$ dans \edris, biais qui est particulièrement marqué lorsque l'incertitude sur la couleur $\sigma_c$ est élevée. Ce problème était déjà connu, mais a été confirmé sur les simulations produites lors de ce stage lorsque la reconstruction est effectuée avec \saltd. Ce n'est pas le cas pour la reconstruction avec \nacl car les erreurs sont sous-estimées pour une raison encore inconnue, mais ce problème pourrait également survenir à l'avenir. La solution retenue est alors de fixer la dispersion intrinsèque à sa valeur utilisée pour les simulations, $\sigma_{int} = 0.1$, puisqu'il s'agit de la dispersion observée expérimentalement.

\subsubsection{Condition d'arrêt du gradient conjugué dans \edris}

Lors de l'utilisation d'\edris, j'ai été amené à me documenter sur le détail de la méthode utilisée. En particulier, l'optimisation utilisée est décrite dans \cite{martens_deep_2010}, et mentionne explicitement que pour un problème linéaire $Ax = b$, les méthodes de gradient conjugué ne minimisent pas l'erreur quadratique $\norm{Ax-b}^2$ mais la forme quadratique $\phi(x) = \frac{1}{2} x^T A x - b^T x$. Bien que ces deux formulations présentent le même minimum global, une solution acceptable mais sous-optimale pour l'un peut s'avérer être mauvaise pour l'autre. En particulier lors d'une exécution de gradient conjugué, si $\phi(x)$ est bien décroissante, $\norm{Ax-b}^2$ fluctue violemment et ne commence à tendre vers 0 qu'à la fin de l'optimisation. Pourtant, les conditions d'arrêt des implémentations de gradient conjugué se basent souvent sur $\norm{Ax-b}^2$ et non $\phi(x)$, potentiellement car cette dernière n'est pas bornée par 0. L'auteur propose alors de modifier la condition d'arrêt pour la conditionner au taux de réduction de $\phi$, moyenné sur $k$ itérations
\begin{equation}
	i > k \qq{et} \phi(x_i) < 0 \qq{et} \frac{\phi(x_i) - \phi(x_{i-k})}{\phi(x_i)} < k \epsilon
\end{equation}

Le problème pour \edris  vient alors de son implémentation. Afin d'accélérer l'évaluation des log-vraisemblance et de leurs dérivées, \edris a été intégralement implémenté en \texttt{JAX}\footnote{\href{https://jax.readthedocs.io/en/latest/}{https://jax.readthedocs.io/en/latest/}}. Ce package propose une ré-implémentation des packages \texttt{python} usuels pour qu'ils soient compatibles avec l'optimisation proposée par \texttt{JAX}, en particulier le package \texttt{scipy} et son implémentation du gradient conjugué. Cependant, son implémentation initiale et la ré-implémentation en \texttt{JAX} utilisent toutes les deux la conditions d'arrêt déconseillée par \cite{martens_deep_2010}, ce qui peut avoir un impact non négligeable sur la convergence d'\edris.

Ce problème demeure à ce jour non résolu d'une part car son effet, s'il est bien présent, n'était pas la cause majeure d'une mauvaise convergence (voir notamment \ref{sec:qc_edris}), et d'autre part car sa résolution est compliquée puisqu'elle nécessite d'ajouter une condition d'arrêt non déterministe à la fonction de gradient conjugué, ce dont \texttt{JAX} ne s’accommode pas trivialement.



\section{Extraction des vitesses particulières}

Les vitesses particulières ainsi obtenues sont représentées en Fig. \ref{fig:vp}. Les vitesses reconstruites sont en très bon accord avec les vitesses injectées : l'erreur moyenne n'est que de $54$km.s$^{-1}$ et l'écart type de l'erreur commise est de 410km.s$^{-1}$, ce qui est faible sachant que les vitesses particulières valent en moyenne $\sim 200$km.s$^{-1}$ et présentent une dispersion de $\sim 300$km.s$^{-1}$.   L'effet de la dispersion intrinsèque sur l'estimation des vitesses particulières est particulièrement visible à haut redshift, car la contribution des vitesses particulières au redshift observé est alors plus faible, et le module de distance évolue plus lentement avec le redshift.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/edris_vp_uchuu_vs_edris.png}
    \caption{Comparaison entre les vitesses particulières injectées (dénommée $pv_{Uchuu}$) et les vitesses particulières reconstruites ($pv_{edris}$). L'augmentation de l'erreur commise avec le redshift (en bas à gauche) est explicable par le fait qu'à haut redshift, l'effet des vitesses particulières est dominé par la dispersion intrinsèque. Par conséquent la dispersion statistique est à tort considérée comme une contribution des vitesses particulières.}
    \label{fig:vp}
\end{figure}


\chapter{Travail futur}
\consignes{prolongements possibles du travail de stage : exposé de ce qu’il reste éventuellement à faire pour que les résultats soient complètement obtenus, et de la façon dont le travail pourra être poursuivi par l’entreprise dans le futur}

\section{Finalisation du travail effectué lors du stage}

\todo{FS8 -> modélisation de la densité de Uchuu, autre simulation basée sur 2M++}

\section{Postérité pour l'entreprise}


La génération des SNe est effectuée à l'aide du module \skysurvey, mais il s'agit d'une nouvelle fonctionnalité qui n'était pas disponible auparavant. Si le tirage utilise actuellement les positions des SNe suivant le catalogue Uchuu BGS, il est possible d'utiliser n'importe quelle autre catalogue de galaxies ou plus généralement n'importe quelle table de coordonnées $(RA,DEC,Z)$. Mon implémentation est actuellement en cours d'intégration au module \skysurvey comme méthode potentielle de tirage des positions, afin de pouvoir être réutilisée pour des analyses futures.

\todo{Mahmoud utilise des simus semblables, intégration implémentée dans george par Jérémy, skysurvey}




\chapter{Bilan personnel}
\consignes{bilan des principaux apprentissages professionnels et leur mise en perspective pour le futur professionnel}

\todo{TODO}



\chapter{Enjeux éthiques}
\consignes{prise de recul sur les enjeux d’ordre éthique}

\section{Éthique de la recherche}

\section{Égalité F/H}

\todo{Toujours une grosse disparité, mais messages/affiches de sensibilisation. Pas de discriminations observées lors du stage.}

\section{Impact environnemental}

\todo{Sensibilisation lors du meeting DESI 2024, supercalculateurs}





\appendix
\chapter{Annexes}
\pagenumbering{roman}

\section{Fonctionnement des modules}

\subsection{\pets}
\label{anx:pets}

PETS a pour but de sélectionner les SN qui sont suffisamment bien échantillonnées pour l'entraînement du modèle par NaCl.
Le fonctionnement de PETS est découpé en 3 étapes, tous utilisant le modèle \saltd déjà entraîné~:
\begin{enumerate}
\item Une sélection des points de données utilisables.
\item La réalisation d'une grille de temps de maximums et des fits associés à chacun de ces temps
\item La sélection des SNe
\end{enumerate}

La sélection des points de données est effectuée en rejetant itérativement les points de données à plus de 3$\sigma$ du fit \saltd. Ensuite, en partant du $t_{max, 0}$ d'un fit complet des points restants, une grille de temps de maximum est réalisé. Elle contient 150 points dans les intervalles $[t_{max, 0} - 20 ; t_{max, 0} - 5]$ et $[t_{max, 0} +5; t_{max, 0} + 20]$ et 1000 points dans l'intervalle $[t_{max, 0} - 5 ; t_{max, 0} + 5]$. Le modèle est alors fitté sur les données en fixant le temps de maximum à un de ces points, puis PETS analyse la courbe $\chi^2(t_{max})$ afin de garder ou rejeter la SN. Une SN est gardée si~:
\begin{enumerate}
\item $\chi^2(t_{max})$ présente un minimum global à $t_{min}$, qui n'est pas aux bords de l'intervalle $[t_{max,0} - 20 ; t_{max, 0} + 20]$
\item L'incertitude sur $t_{min}$ est inférieure à 1 jour.
\item L'écart entre les incertitudes positives et négatives à 3$\sigma$ est inferieur à $0.3$, ce qui caractérise la symétrie de $\chi^2(t_{max})$ autour de $t_{min}$.
\item $\chi^2(t_{max})$ ne présente pas d'autres minimums à 8$\sigma$ de ce minimum global, \textit{i.e.} il n'y a pas d'autres minimums proches susceptibles de coincer le fit de NaCl.
\item Les valeurs de $x_1$ et $c$ fittées sont respectivement inférieures à 4 et 2, ce qui impose que le fit ne décrit pas une SN extrêmement distordue (bien qu'elle soit toujours décrite par le modèle).
\end{enumerate}

Une version mise à jour de PETS utilisant un modèle NaCl pré-entraîné à la place de \saltd est en cours de développement, mais je ne l'ai pas utilisée.


\subsection{\nacl}
\label{anx:nacl}

\subsubsection{Modélisation des SNe}

\nacl consiste en une implémentation d'un modèle type \texttt{SALT}, avec quelques améliorations. Tout comme \texttt{SALT}, le flux émis par les SNe dans leurs référentiels propres est modélisé par une surface spectrale $\mcl S(\lambda, p)$, où $\lambda$ est la longueur d'onde d'émission dans le référentiel de la SN, $p$, la phase, est le temps depuis le maximum d'émission en bande B.
De cette surface de flux, on peut déduire dans notre référentiel à la fois le flux photométrique, c'est-à-dire le flux observé dans un filtre de bande passante $T(\lambda)$ à une date donnée, et les spectres émis en redshiftant cette surface~:
\begin{gather}
	\phi_{phot} = X_0 \frac{1}{1+z} \int \mcl S\qty(\frac{\lambda}{1+z}, p) \frac{\lambda}{hc} T(\lambda) \dd{\lambda} \label{eq:phot}\\
	\phi_{spec}  = X_0 \frac{1}{1+z} \mcl S(\lambda, p) \label{eq:spec}
\end{gather}
où $X_0 = \Phi_B \frac{(10\text{pc}^2}{d_L(z)^2}$ est l'amplitude, avec $\Phi_B$ la luminosité maximale en bande B normalisée pour une SN à 10 pc, et $d_L(z)$ est la distance de luminosité. Toute l'information sur la cosmologie est donc encodée dans $X_0$ via la distance de luminosité.
Pour réduire le temps de calcul, l'équation \ref{eq:phot} est ré-exprimée en intégrant dans le référentiel de la SN, ce qui équivaut à blueshifter les filtres plutôt que de redshifter la surface spectrale~:
\begin{equation}
	\phi_{phot} = X_0 (1+z) \int \mcl S(\lambda, p) \frac{\lambda}{hc} T((1+z)\lambda) \dd{\lambda}
\end{equation}

La surface spectrale est décrite de la même manière que pour \saltd avec la paramétrisation par SN suivante
\begin{equation}
    \mcl S (p, \lambda) = \qty[M_0(p, \lambda) + X_1 M_1(p, \lambda)] \times \exp[0..4 c CL(\lambda)]
\end{equation}
où $X_1$ et $c$ sont les paramètres de stretch et couleurs propre à la SN considérée.

Les modèles $M_0(p, \lambda)$ et $M_1(p, \lambda)$ sont exprimés sur une base de splines, et représentés par un vecteur de coefficients $\theta$. Cette compression facilite leurs évaluations et les calculs de leurs dérivées. Les modèles sont ainsi évaluables en tout point ce qui permet de s'affranchir des effets indésirables introduits par une discrétisation (nécessitée d'interpoler, moins bonne précision sur les dérivées,...) .

\subsubsection{Différences entre NaCl et SALT2.4}

Les différences entre \nacl et \saltd ne sont pas liées au modèle de SNe lui-même, mais à des choix dans les paramètres ajustés lors de l'entraînement~:
\begin{itemize}
\item Le temps du maximum $t_{max}$ n'est plus déterminé avant entraînement et fixé, mais est laissé comme paramètre libre. Autrement dit, les points de donnés dans l'espace des phases $(\lambda, p)$ sont susceptibles de bouger selon $p$, puisque décaler la date de maximum revient à décaler d'autant tous les points de mesures.
\item La calibration des filtres est intégrée dans le modèle ce qui permet d'estimer directement son impact sur les paramètres reconstruits, plutôt que d'entraîner le modèle sur différents lots avec des calibrations variables.
\item En conséquence de la différence précédente, le modèle d'erreur, c'est à dire l'incertitude du modèle sur les valeurs de flux, est également ajusté lors de l'entraînement plutôt qu'estimé a posteriori.
\end{itemize}
Le lecteur attentif aura noté que les amplitudes et stretch de SALT étaient notées avec des minuscules $x_0$ et $x_1$, tandis que celles de NaCl sont notées avec des majuscules $X_0$ et $X_1$. Cette différence est intentionnelle, puisque du fait des différences dans les contraintes et l'entraînement, rien ne garantit que ces paramètres soient exactement les mêmes, ce qui n'est d'ailleurs pas le cas dans les faits (\textcolor{red}{cf Résultats}).

\subsubsection{Contraintes}

En conséquence de ces choix, les contraintes imposées sur les modèles diffèrent de celles utilisées pour entraîner \saltd. Une des idées fondamentales de ces contraintes est que $M_0$ doit décrire la SN moyenne, et que tous les termes autour ne doivent être que des corrections. Cela ce traduit par exemple dans la normalisation des modèles, dégénérée avec $X_0$, en imposant
\begin{gather}
	\int M_0(p=0,\lambda) \frac{\lambda}{hc} B(\lambda) \dd{\lambda} = 1\\
	\int M_0(p=0,\lambda) \frac{\lambda}{hc} B(\lambda) \dd{\lambda} = 0
\end{gather}
De même, les couleurs et stretchs sont contraints pour être de moyenne nulle, par
\begin{gather}
	\sum_{i=1}^{N_{SN}} c_i = 0\\
	\sum_{i=1}^{N_{SN}} X_{1,i} = 0\\
\end{gather}
À cela s'ajoute une dégénérescence liée aux normalisations des $X_1$ relativement à $M_1$, qui est levée en imposant une contrainte sur la variance des $X_1$
\begin{gather}
	\frac{1}{N_{SN}} \sum_{i=1}^{N_{SN}} (X_{1,i} - \ev{X_1})^2 = 1
\end{gather}

La variabilité du temps de maximum introduit également une dégénérescence avec les modèles $M_0$ et $M_1$. Cette dégénérescence est brisée en imposant aux modèles de présenter un maximum en bande B à $p=0$~:
\begin{gather}
	\eval{\dv{p} \int M_0(p,\lambda) \frac{\lambda}{hc} B(\lambda) \dd{\lambda}}_{p=0} = 0\\
	\eval{\dv{p} \int M_1(p,\lambda) \frac{\lambda}{hc} B(\lambda) \dd{\lambda}}_{p=0} = 0
\end{gather}

Toutes ces contraintes sont exprimées sous la forme
\begin{equation}
	C(\theta) - \gamma = 0
\end{equation}
où $\theta$ représente le vecteur des paramètres des modèles, et intégrées dans la vraisemblance dans un terme
\begin{equation}
	\mu_{cons} C(\theta)^T C(\theta)
\end{equation}

Enfin, un terme de régularisation est introduit sous la forme
\begin{equation}
	\mu_{reg} \qty(\sum_{k,l} \theta^2_{k,l} + \sum_{k,l} (\theta_{k+1, l} - \theta_{k,l})^2)
\end{equation}
afin de lever d'éventuelles dégénérescences non identifiées, en faisant tendre les paramètres du modèle vers 0 lorsqu'ils ne sont pas contraint par des points de données via premier terme et en limitant l'amplitude des fluctuations des modèles via le second terme.


 \subsubsection{Entraînement}

L'entraînement a lieu en 3 étapes après initialisation des modèles $M_0$ et $M_1$ sur les modèles de \saltd. Une première estimation des paramètres de chaque SN $(x_0, x_1, c, t_{max})$ est réalisée à modèles et modèle d'erreur fixés. Autrement dit, cette première estimation revient à ré-estimer les paramètres des SNe en utilisant les mêmes modèles que \saltd. Ensuite, les paramètres des SNe sont fixés aux valeurs obtenues, et seul le modèle d'erreur est laissé libre et ajusté. Pour finir, tous les paramètres sont relâchés et le modèle est ajusté en même temps que les paramètres des SNe et le modèle d'erreur. C'est lors de cette dernière étape que les contraintes décrites précédemment sont effectivement appliquées au modèle, puisque les modèles de \saltd utilisaient d'autres contraintes.

Ce choix vient du fait qu'un entraînement sur tous les paramètres serait trop coûteux et qu'il pourrait se bloquer plus facilement dans un minimum local.

\subsection{\edris}
\label{anx:edris}

Le module \edris réalise un diagramme de Hubble en ajustant un modèle cosmologique aux modules de distances de SNe en prenant en compte les effets de sélections tel que le biais de Malmquist, et en estimant directement la matrice de covariance des paramètres. Il utilise pour cela la méthode d'optimisation dite \textit{Hessian-free} décrite dans \cite{martens_deep_2010}. Cette méthode présente l'avantage de ne nécessiter que des calculs de gradients afin de déterminer une approximation de la hessienne, et non de la hessienne complète.

\todo{MORION}





\section{Méthode de l'analyse jointe $f\sigma_8$}
\label{anx:fs8}

Comme mentionné précédemment, l'analyse $f\sigma_8$ se base sur la méthode de comparaison entre les vitesses particulières et celles obtenues à partir du champ de densité, plus précisément sur la méthodologie de \cite{stahl_peculiar-velocity_2021}.

La relation entre le champ de vitesse et le contraste de densité des galaxies $\delta_g = \frac{\rho_g}{\ov{\rho_g}} - 1$ est donné par~:
\begin{equation}
    \vb v (\vb r) = \frac{H_0 \mcl B}{4 \pi} \int_0^{R_{max}} \dd[3]{\vb r'} \delta_g(\vb r') \frac{\vb r' - \vb r}{\abs{\vb r' - \vb r}} + \vb V_{ext}
\end{equation}
où $\mcl B =\frac{f}{b}$, avec $b$ le biais entre le contraste de densité des galaxies et celui de la matière noire ($\delta_g = b \delta_{DM}$), $R_{max}$ est la distance maximale couverte par le catalogue, et $\vb V_{ext}$ est le flux cohérent causé par des structures à l'extérieur du catalogue.

Les vitesses obtenues à partir du champ de densité sont alors ajustées sur celles des vitesses particulières issues des SN à partir d'une méthode \textit{Forward Likelihood}. Les paramètres $\mcl B$ et $\vb V_{ext}$ ainsi que d'autres paramètres de nuisance liés aux observations $\vb \Theta$ (notamment les paramètres du modèle de Tripp $\alpha$, $\beta$, $M_b$ et $\sigma_{int}$, voir Eq. \ref{eq:tripp}) sont reliés aux paramètres observables des SN $\{\vb x_i\}$ via la probabilité conditionnelle $\mcl P(\mcl B, \vb V_{ext}, \vb \Theta | \vb x_i)$, qui peut être exprimée à l'aide du théorème de Bayes (\cite{stahl_peculiar-velocity_2021})
\begin{equation}
    \mcl P(\mcl B, \vb V_{ext}, \vb \Theta | \vb x_i) \propto \mcl P(\vb x_i|\mcl B, \vb V_{ext}, \vb \Theta) \mcl P(\mcl B, \vb V_{ext}, \vb \Theta)
\end{equation}
On cherche donc à reconstruire les distributions contraintes par les observations des paramètres de biais galactique, de flux cohérent, et de nuisance ($\mcl P(\mcl B, \vb V_{ext}, \vb \Theta | \vb x_i)$) à l'aide de la distribution des SN de paramètres  $\vb x_i$ compte tenu du biais galactique, du flux cohérent, et des paramètres de nuisance ($\mcl P(\vb x_i | \mcl B, \vb V_{ext}, \vb \Theta )$)  et d'un prior sur la distributions de ces paramètres ($\mcl P(\mcl B, \vb V_{ext}, \vb \Theta)$).

Les priors sur les paramètres de nuisance du modèle de Tripp sont non restrictifs, ils imposent simplement $\alpha, \beta > 0$ et $M < 0$, tandis que la dispersion intrinsèque $\sigma_{int}$ (cf. Eq. \ref{eq:tripp}) est log-normale avec un pic à $0.15$ mag.

Pour corriger d'éventuelles inhomogénéités selon différentes lignes de visées, on doit introduire de plus une distribution radiale
\begin{equation}
    \mcl P(\vb r|\vb \Theta) = \frac{1}{\mcl N(\vb \Theta)} r^2 \exp{- \frac{\qty[\mu(\vb r) - \mu(\vb\Theta)]^2}{2 \sigma_\mu^2(\vb \Theta)}} \qty[1 + \delta_g(\vb r)]
\end{equation}
où $\vb r$ est un vecteur comobile pointant sur la sphère célèste, $\mcl N(\vb \Theta)$ est un coefficient de normalisation, $\mu$ est le module de distance, et $\sigma_\mu$ est l'erreur sur le module de distance causée par la dispersion intrinsèque des SNe. On obtient alors la vraisemblance suivante
\begin{equation}
    \mcl P(\vb x_i|\mcl B, \vb V_{ext}, \vb \Theta) = \int_0^{R_{max}} \dd{r} \mcl P (\vb x_i|\vb r, \mcl B, \vb V_{ext}, \vb \Theta) \mcl P(\vb r|\vb \Theta)
\end{equation}
avec
\begin{equation}
     \mcl P (\vb x_i|\vb r, \mcl B, \vb V_{ext}, \vb \Theta) = \frac{1}{\sqrt{2 \pi \sigma_\nu^2}} \exp{-\frac{\qty[c z_{obs} - cz_{pred}(\vb r, \mcl B, \vb V_{ext})]^2}{2\sigma_\nu^2}}
\end{equation}
$z_{obs}$ est le redshift observé de la SN, et $z_{pred}$ est la prédiction de ce redshift à partir du redshift cosmologique et des vitesses reconstruites grâce à
\begin{equation}
    1 + z_{pred}(\vb r, \mcl B, \vb V_{ext}) = (1 + z_{cosmo}(\vb r)) \qty[1 + \frac{1}{c} (\mcl B \vb v(r) + \vb V_{ext}) \cdot \vu r]
\end{equation}
$\sigma_\nu$ peut être soit ajusté avec les autres paramètres de nuisance comme dans \cite{stahl_peculiar-velocity_2021}, auquel cas il suit une dispersion gaussienne, soit fixé comme dans \cite{boruah_cosmic_2020}.

Finalement, la log-vraisemblance s'écrit
\begin{equation}
    \mcl L = \ln \mcl P(\mcl B, \vb V_{ext}, \vb \Theta) + \sum_i P(\vb x_i|\mcl B, \vb V_{ext}, \vb \Theta)
\end{equation}
Cette vraisemblance est alors explorée avec une Chaîne de Markov Monte Carlo, et produit des résultats comme ceux de \cite{stahl_peculiar-velocity_2021} représentés en Fig. \ref{fig:stahl}, qui sont en très bon accord avec d'autres analyses $f\sigma_8$ et ont une meilleure précision que les analyses $f\sigma_8$ utilisant seulement les RSD.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Stahl_fig_6.png}
    \caption{Paramètre $S_8 = f \sigma_8/(0.3)^{0.55}$ reconstruit par diverses méthodes. Les lignes noires représentent la valeur et l'incertitude de la valeur reconstruite par \cite{stahl_peculiar-velocity_2021}. Crédit : B. Stahl}
    \label{fig:stahl}
\end{figure}


\printbibliography

\end{document}
